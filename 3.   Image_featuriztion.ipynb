{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"3.   Image_featuriztion.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"8kkMAqfhiBoU","colab_type":"text"},"source":["# Image featurization ( part 3)\n","\n","### Converting images to feature using transfer learning.\n","####  This script has used data11.csv file that is generated from script-2( part 2)\n","####   This script will product feature.npy, lst.npy file that will be used in part 4"]},{"cell_type":"code","metadata":{"id":"3-kNOBxPiBoX","colab_type":"code","colab":{}},"source":["#import all the necessary packages.\n","\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import warnings\n","import nltk\n","import math\n","import time\n","import re\n","import os\n","\n","import numpy as np\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dropout, Flatten, Dense\n","from keras import applications\n","from sklearn.metrics import pairwise_distances\n","import matplotlib.pyplot as plt\n","import requests\n","from PIL import Image\n","import pandas as pd\n","import pickle\n","import glob\n","from keras.preprocessing import image\n","#from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","from vgg16 import VGG16\n","from keras.preprocessing import image\n","\n","from keras.preprocessing import image\n","from keras.applications.imagenet_utils import preprocess_input\n","from keras.layers import Dense, Activation, Flatten\n","from keras.layers import merge, Input\n","from keras.models import Model\n","\n","from datetime import datetime\n","from IPython.display import Image, display"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHnsGqGQiBoc","colab_type":"code","colab":{}},"source":["import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","import tensorflow as tf\n","import keras\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth=True\n","sess = tf.Session(config=config)\n","keras.backend.set_session(sess)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c037BpCxiBof","colab_type":"code","colab":{}},"source":["data11 = pd.read_csv('data11.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"axKhLuGziBoj","colab_type":"code","colab":{},"outputId":"4aadb582-3a97-4dd3-a846-19deeb772fbf"},"source":["data11.head(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>productId</th>\n","      <th>title</th>\n","      <th>imageUrlStr</th>\n","      <th>mrp</th>\n","      <th>categories</th>\n","      <th>size</th>\n","      <th>productBrand</th>\n","      <th>path</th>\n","      <th>exists</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TOPE9ABBZU3HZRHN</td>\n","      <td>Citrine Casual Short Sleeve Printed Women's Pi...</td>\n","      <td>http://img.fkcdn.com/image/top/r/h/n/1-1-wwtpw...</td>\n","      <td>1099</td>\n","      <td>Apparels&gt;Women&gt;Western Wear&gt;Shirts, Tops &amp; Tun...</td>\n","      <td>S</td>\n","      <td>Citrine</td>\n","      <td>images/TOPE9ABBZU3HZRHN.jpeg</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TOPE9ABBBTJYDSQE</td>\n","      <td>Citrine Casual Short Sleeve Printed Women's Pi...</td>\n","      <td>http://img.fkcdn.com/image/top/r/h/n/1-1-wwtpw...</td>\n","      <td>1099</td>\n","      <td>Apparels&gt;Women&gt;Western Wear&gt;Shirts, Tops &amp; Tun...</td>\n","      <td>M</td>\n","      <td>Citrine</td>\n","      <td>images/TOPE9ABBBTJYDSQE.jpeg</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          productId                                              title  \\\n","0  TOPE9ABBZU3HZRHN  Citrine Casual Short Sleeve Printed Women's Pi...   \n","1  TOPE9ABBBTJYDSQE  Citrine Casual Short Sleeve Printed Women's Pi...   \n","\n","                                         imageUrlStr   mrp  \\\n","0  http://img.fkcdn.com/image/top/r/h/n/1-1-wwtpw...  1099   \n","1  http://img.fkcdn.com/image/top/r/h/n/1-1-wwtpw...  1099   \n","\n","                                          categories size productBrand  \\\n","0  Apparels>Women>Western Wear>Shirts, Tops & Tun...    S      Citrine   \n","1  Apparels>Women>Western Wear>Shirts, Tops & Tun...    M      Citrine   \n","\n","                           path  exists  \n","0  images/TOPE9ABBZU3HZRHN.jpeg    True  \n","1  images/TOPE9ABBBTJYDSQE.jpeg    True  "]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"xTvZ7ur1iBon","colab_type":"text"},"source":["# I have used transfer learning for feature extraction from image. Used VGG net pretrained on ImageNet Data"]},{"cell_type":"code","metadata":{"id":"G-qF66sIiBoo","colab_type":"code","colab":{},"outputId":"b0e38042-71e7-4416-ddb9-6e264ddd3689"},"source":["image_input = Input(shape=(150, 150, 3))\n","\n","model = VGG16(input_tensor=image_input, include_top= False,weights='imagenet')\n","model.layers[0].trainable = False\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 150, 150, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-qB_deg9iBos","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcUF8oo7iBou","colab_type":"text"},"source":["# Save the productId of all images in the lst( list) , that I have downloaded."]},{"cell_type":"code","metadata":{"id":"EMRLhvEjiBov","colab_type":"code","colab":{}},"source":["path = 'images/*'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxbnxVjQiBoy","colab_type":"code","colab":{}},"source":["lst = []\n","productId = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5EFuXYo7iBo1","colab_type":"text"},"source":["\n","for img_path in glob.glob(path):\n","    img = img_path.split('/')[1]\n","    img = img.split('.')[0]\n","    lst1 = []\n","    lst1.append(img)\n","    df1 = pd.DataFrame(lst1, columns=['productId'])\n","    productId = productId.append(df1)"]},{"cell_type":"markdown","metadata":{"id":"K0ePzE13iBo2","colab_type":"text"},"source":["cc = pd.DataFrame(productId.values)"]},{"cell_type":"markdown","metadata":{"id":"AAs-T4jyiBo3","colab_type":"text"},"source":["### This If condition for constraining the number of images. Here I have put the condition infinity."]},{"cell_type":"code","metadata":{"id":"ij7Ga6hziBo4","colab_type":"code","colab":{}},"source":["idxx = 0\n","\n","for img_path in glob.glob(path):\n","    if idxx < 5000000:                  # This If condition for constraining the number of images. Here I have put the condition infinity.\n","        img = img_path.split('/')[1]\n","        img = img.split('.')[0]\n","        lst.append(img)\n","    idxx = idxx+1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"STfRaDhKiBo7","colab_type":"code","colab":{},"outputId":"0e057b3f-649f-423a-ef1a-8bbe28caccec"},"source":["idxx   # 346205 images."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["346205"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"tW-LaJHKiBo-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QsR3zPfiBpB","colab_type":"code","colab":{}},"source":["df_asins = list(data11['productId']) "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sayJYPdmiBpE","colab_type":"text"},"source":["http://chrisschell.de/2018/02/01/how-to-efficiently-deal-with-huge-Numpy-arrays.html\n","### save the downloaed images productId in the lst.npy"]},{"cell_type":"code","metadata":{"id":"X3ocvrB3iBpF","colab_type":"code","colab":{}},"source":["\n","np.save('lst.npy',lst)  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ia-9P_OhiBpI","colab_type":"text"},"source":["#### Initialize the numpy ndarray for saving the extracted features of 346205  images. Each image has extracted to 8192 length feature vector"]},{"cell_type":"code","metadata":{"id":"2mAjOGDfiBpJ","colab_type":"code","colab":{}},"source":["\n","resnet_feature_list = np.ones((346205,8192)) # prefilled array\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUfW8tsxiBpM","colab_type":"code","colab":{},"outputId":"fb3d25a8-6667-4441-95c3-0e5b516de3e5"},"source":["resnet_feature_list"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       ...,\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"KkmqWJ97iBpP","colab_type":"code","colab":{}},"source":["def extract_vector111(path):\n","    #resnet_feature_list = []\n","    idx = 0\n","    for img_path in glob.glob(path):\n","        if idx < 50000000:\n","            if idx% 1000 == 0:\n","                print(idx)\n","            img = image.load_img(img_path, target_size=(150, 150))\n","            img_data = image.img_to_array(img)\n","            img_data = np.expand_dims(img_data, axis=0)\n","            img_data = preprocess_input(img_data)\n","            resnet_feature = model.predict(img_data)\n","            resnet_feature_np = np.array(resnet_feature)\n","            resnet_feature_list[idx] = resnet_feature_np.flatten()\n","        \n","        idx = idx+1  \n","    return resnet_feature_list\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IlKz9V7MiBpT","colab_type":"code","colab":{},"outputId":"4db13f4f-606d-49cb-baa1-8a2beab65f9b"},"source":["start = datetime.now()\n","features = extract_vector111(path)\n","print(\"\\nTime Taken: \",datetime.now() - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n","31000\n","32000\n","33000\n","34000\n","35000\n","36000\n","37000\n","38000\n","39000\n","40000\n","41000\n","42000\n","43000\n","44000\n","45000\n","46000\n","47000\n","48000\n","49000\n","50000\n","51000\n","52000\n","53000\n","54000\n","55000\n","56000\n","57000\n","58000\n","59000\n","60000\n","61000\n","62000\n","63000\n","64000\n","65000\n","66000\n","67000\n","68000\n","69000\n","70000\n","71000\n","72000\n","73000\n","74000\n","75000\n","76000\n","77000\n","78000\n","79000\n","80000\n","81000\n","82000\n","83000\n","84000\n","85000\n","86000\n","87000\n","88000\n","89000\n","90000\n","91000\n","92000\n","93000\n","94000\n","95000\n","96000\n","97000\n","98000\n","99000\n","100000\n","101000\n","102000\n","103000\n","104000\n","105000\n","106000\n","107000\n","108000\n","109000\n","110000\n","111000\n","112000\n","113000\n","114000\n","115000\n","116000\n","117000\n","118000\n","119000\n","120000\n","121000\n","122000\n","123000\n","124000\n","125000\n","126000\n","127000\n","128000\n","129000\n","130000\n","131000\n","132000\n","133000\n","134000\n","135000\n","136000\n","137000\n","138000\n","139000\n","140000\n","141000\n","142000\n","143000\n","144000\n","145000\n","146000\n","147000\n","148000\n","149000\n","150000\n","151000\n","152000\n","153000\n","154000\n","155000\n","156000\n","157000\n","158000\n","159000\n","160000\n","161000\n","162000\n","163000\n","164000\n","165000\n","166000\n","167000\n","168000\n","169000\n","170000\n","171000\n","172000\n","173000\n","174000\n","175000\n","176000\n","177000\n","178000\n","179000\n","180000\n","181000\n","182000\n","183000\n","184000\n","185000\n","186000\n","187000\n","188000\n","189000\n","190000\n","191000\n","192000\n","193000\n","194000\n","195000\n","196000\n","197000\n","198000\n","199000\n","200000\n","201000\n","202000\n","203000\n","204000\n","205000\n","206000\n","207000\n","208000\n","209000\n","210000\n","211000\n","212000\n","213000\n","214000\n","215000\n","216000\n","217000\n","218000\n","219000\n","220000\n","221000\n","222000\n","223000\n","224000\n","225000\n","226000\n","227000\n","228000\n","229000\n","230000\n","231000\n","232000\n","233000\n","234000\n","235000\n","236000\n","237000\n","238000\n","239000\n","240000\n","241000\n","242000\n","243000\n","244000\n","245000\n","246000\n","247000\n","248000\n","249000\n","250000\n","251000\n","252000\n","253000\n","254000\n","255000\n","256000\n","257000\n","258000\n","259000\n","260000\n","261000\n","262000\n","263000\n","264000\n","265000\n","266000\n","267000\n","268000\n","269000\n","270000\n","271000\n","272000\n","273000\n","274000\n","275000\n","276000\n","277000\n","278000\n","279000\n","280000\n","281000\n","282000\n","283000\n","284000\n","285000\n","286000\n","287000\n","288000\n","289000\n","290000\n","291000\n","292000\n","293000\n","294000\n","295000\n","296000\n","297000\n","298000\n","299000\n","300000\n","301000\n","302000\n","303000\n","304000\n","305000\n","306000\n","307000\n","308000\n","309000\n","310000\n","311000\n","312000\n","313000\n","314000\n","315000\n","316000\n","317000\n","318000\n","319000\n","320000\n","321000\n","322000\n","323000\n","324000\n","325000\n","326000\n","327000\n","328000\n","329000\n","330000\n","331000\n","332000\n","333000\n","334000\n","335000\n","336000\n","337000\n","338000\n","339000\n","340000\n","341000\n","342000\n","343000\n","344000\n","345000\n","346000\n","\n","Time Taken:  2:26:07.277281\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6jEaZ7_BiBpW","colab_type":"code","colab":{}},"source":["np.save('features.npy',features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOkDxEY9iBpb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juA3kQ9viBph","colab_type":"text"},"source":["### This script file has generated 2 file. one is list of productid of the downloaded images( lst.npy) and other is numpy array of images extracted features. (features.npy)"]},{"cell_type":"code","metadata":{"id":"mth9MzzOiBpj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}